# tests to run:

using batch norm: worse

using l1 reg: worse

using dropout in generator: worse

using xavier initialization: same?

different weight initialization params

different learning rates

different hidden unit sizes

different batch sizes